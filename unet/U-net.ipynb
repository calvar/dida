{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f15849",
   "metadata": {},
   "source": [
    "<img src=\"unet_arch.png\" title=\"U-net arch.\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3fd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import natsort\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecab26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'#'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad2ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "H = 128#256\n",
    "W = 128#256\n",
    "size = (H,W)\n",
    "batch_size = 2\n",
    "num_epochs = 50\n",
    "learn_rate = 1.e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a189d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed for reproducibility\n",
    "def seeding(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6696bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeding(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f1a36",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd5d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, main_dir, img_transform, msk_transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.img_transform = img_transform\n",
    "        self.msk_transform = msk_transform\n",
    "        self.img_dir = main_dir+'images/'\n",
    "        self.msk_dir = main_dir+'masks/'\n",
    "        all_images = os.listdir(self.img_dir)[:20] #OJO!!!!!\n",
    "        all_masks = os.listdir(self.msk_dir)[:20] #OJO!!!!!\n",
    "        self.images = natsort.natsorted(all_images)\n",
    "        self.masks = natsort.natsorted(all_masks)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.img_dir, self.images[idx])\n",
    "        img = Image.open(img_loc).convert('RGB')\n",
    "        tensor_img = self.img_transform(img)\n",
    "        msk_loc = os.path.join(self.msk_dir, self.masks[idx])\n",
    "        msk = Image.open(msk_loc).convert('L') \n",
    "        tensor_msk = self.msk_transform(msk)\n",
    "        tensor_msk /= 255 #Target must be between 0 and 1 \n",
    "        return tensor_img, tensor_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110319ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a transform object that takes the data to pytorch tensor form and normalizes it\n",
    "img_transform = Compose( [Resize(size), ToTensor(), Normalize(mean=(0.5,),std=(0.5,))] );\n",
    "msk_transform = Compose( [Resize(size), ToTensor()] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f56ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(main_dir='./data/train/',img_transform=img_transform,msk_transform=msk_transform)\n",
    "valid_set = CustomDataset(main_dir='./data/test/',img_transform=img_transform,msk_transform=msk_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0170c891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:\n",
      "Train: 20 Validation: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\\nTrain: {0} Validation: {1}\".format(len(train_set),len(valid_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d47967f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 128, 128]), torch.Size([1, 128, 128]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, msk = train_set[0]\n",
    "img.shape, msk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef42517",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset = train_set,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 2\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset = valid_set,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472eda3f",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0439e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block with 2 succesive convolutions (with same padding in this case to keep the borders)\n",
    "class ConvBlock(nn.Module):\n",
    "    #Receives number of input and output channels\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(ConvBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1) #same\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1) #same\n",
    "        self.bn = nn.BatchNorm2d(out_c) #Normalize each batch (zero mean->no bias)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #First convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        #Second convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "#Block that performs the pool operation reducing the size in half.\n",
    "#It also returns the original input to do skip-layer connection\n",
    "# to the decoder.\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.conv = ConvBlock(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "    \n",
    "#Block that performs a transpose convolution to upsample the input.\n",
    "#Use instead of pre-defined interpolation so that parameter\n",
    "# learning also takes place.\n",
    "#It also adds a crop of the corresponding encoder output to\n",
    "# make the skip connection. In this case no cropping is needed,\n",
    "# as the layers have the same size because of same padding.\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = ConvBlock(2*out_c, out_c) #in channels are x2 because of the concatenation\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], axis=1) #Concatenate along the channel dimension\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "#Our network will take an rgb image\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet,self).__init__()\n",
    "        #Encoder\n",
    "        self.e1 = EncoderBlock(3, 64) #Input rgb image and use 64 filters\n",
    "        self.e2 = EncoderBlock(64, 128) #reduce image size by half and duplicate no. of filters\n",
    "        self.e3 = EncoderBlock(128, 256) #reduce image size by half and duplicate no. of filters\n",
    "        self.e4 = EncoderBlock(256, 512) #reduce image size by half and duplicate no. of filters\n",
    "        \n",
    "        #Bottleneck\n",
    "        self.b = ConvBlock(512, 1024) #Convolution without pooling\n",
    "        \n",
    "        #Decoder\n",
    "        self.d1 = DecoderBlock(1024, 512) #upscale the image and reduce by half the no. of filters\n",
    "        self.d2 = DecoderBlock(512, 256) #upscale the image and reduce by half the no. of filters\n",
    "        self.d3 = DecoderBlock(256, 128) #upscale the image and reduce by half the no. of filters\n",
    "        self.d4 = DecoderBlock(128, 64) #upscale the image and reduce by half the no. of filters\n",
    "        \n",
    "        #Classifier\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=1, padding=0) #output channels is the number of output classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s1, x = self.e1(x)\n",
    "        s2, x = self.e2(x)\n",
    "        s3, x = self.e3(x)\n",
    "        s4, x = self.e4(x)\n",
    "        \n",
    "        x = self.b(x)\n",
    "        \n",
    "        x = self.d1(x, s4)\n",
    "        x = self.d2(x, s3)\n",
    "        x = self.d3(x, s2)\n",
    "        x = self.d4(x, s1)\n",
    "       \n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70d3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a62ea",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56c803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From https://github.com/nikhilroxtomar/Retina-Blood-Vessel-Segmentation-in-PyTorch/blob/main/UNET/loss.py\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32b5e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary cross entropy\n",
    "loss_fn = DiceBCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4918e4",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "770222c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06839f",
   "metadata": {},
   "source": [
    "## Training epoch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41239f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    \n",
    "    #Use an enumeration instead of an iterator to track the batch index and do reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        #training instances are input + mask pairs\n",
    "        inputs, masks = data\n",
    "        inputs, masks = inputs.to(device, dtype=torch.float32), masks.to(device, dtype=torch.float32)\n",
    "        \n",
    "        #zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        ##*****************************************\n",
    "        #for layer in model.children():\n",
    "        #    if isinstance(layer, nn.Conv2d):\n",
    "        #        print(torch.isfinite(layer.state_dict()['weight'].grad))\n",
    "        ##*****************************************\n",
    "        \n",
    "        #compute loss and its gradient\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        loss.backward()\n",
    "        \n",
    "        #adjust the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        #gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 #loss per batch\n",
    "            print(' batch {} loss: {}'.format(i+1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0\n",
    "        \n",
    "        del inputs\n",
    "        del masks\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab8840",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "306005f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09c7045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      " batch 10 loss: 1.5884505033493042\n",
      "LOSS train 1.5884505033493042 valid 1.7120850086212158\n",
      "EPOCH 2\n",
      " batch 10 loss: 1.5169926524162292\n",
      "LOSS train 1.5169926524162292 valid 2.0070128440856934\n",
      "EPOCH 3\n",
      " batch 10 loss: 1.444407308101654\n",
      "LOSS train 1.444407308101654 valid 2.1096577644348145\n",
      "EPOCH 4\n",
      " batch 10 loss: 1.385660457611084\n",
      "LOSS train 1.385660457611084 valid 2.438966751098633\n",
      "EPOCH 5\n",
      " batch 10 loss: 1.3515211343765259\n",
      "LOSS train 1.3515211343765259 valid 1.5657553672790527\n",
      "EPOCH 6\n",
      " batch 10 loss: 1.3362690091133118\n",
      "LOSS train 1.3362690091133118 valid 2.3851513862609863\n",
      "EPOCH 7\n",
      " batch 10 loss: 1.3271088123321533\n",
      "LOSS train 1.3271088123321533 valid 1.6018577814102173\n",
      "EPOCH 8\n",
      " batch 10 loss: 1.3113896608352662\n",
      "LOSS train 1.3113896608352662 valid 1.5519403219223022\n",
      "EPOCH 9\n",
      " batch 10 loss: 1.3009114861488342\n",
      "LOSS train 1.3009114861488342 valid 1.5311729907989502\n",
      "EPOCH 10\n",
      " batch 10 loss: 1.2927854776382446\n",
      "LOSS train 1.2927854776382446 valid 1.5210115909576416\n",
      "EPOCH 11\n",
      " batch 10 loss: 1.284339714050293\n",
      "LOSS train 1.284339714050293 valid 1.520445704460144\n",
      "EPOCH 12\n",
      " batch 10 loss: 1.2790295004844665\n",
      "LOSS train 1.2790295004844665 valid 1.511411428451538\n",
      "EPOCH 13\n",
      " batch 10 loss: 1.2728177666664124\n",
      "LOSS train 1.2728177666664124 valid 1.5059024095535278\n",
      "EPOCH 14\n",
      " batch 10 loss: 1.2663351774215699\n",
      "LOSS train 1.2663351774215699 valid 1.5016719102859497\n",
      "EPOCH 15\n",
      " batch 10 loss: 1.2608219981193542\n",
      "LOSS train 1.2608219981193542 valid 1.4909011125564575\n",
      "EPOCH 16\n",
      " batch 10 loss: 1.2550986766815186\n",
      "LOSS train 1.2550986766815186 valid 1.4761327505111694\n",
      "EPOCH 17\n",
      " batch 10 loss: 1.2509244918823241\n",
      "LOSS train 1.2509244918823241 valid 1.4758102893829346\n",
      "EPOCH 18\n",
      " batch 10 loss: 1.2456471920013428\n",
      "LOSS train 1.2456471920013428 valid 1.4695119857788086\n",
      "EPOCH 19\n",
      " batch 10 loss: 1.2406420469284059\n",
      "LOSS train 1.2406420469284059 valid 1.4605321884155273\n",
      "EPOCH 20\n",
      " batch 10 loss: 1.2358936548233033\n",
      "LOSS train 1.2358936548233033 valid 1.4528613090515137\n",
      "EPOCH 21\n",
      " batch 10 loss: 1.2329654216766357\n",
      "LOSS train 1.2329654216766357 valid 1.4683310985565186\n",
      "EPOCH 22\n",
      " batch 10 loss: 1.2293456435203551\n",
      "LOSS train 1.2293456435203551 valid 1.463882327079773\n",
      "EPOCH 23\n",
      " batch 10 loss: 1.2239484906196594\n",
      "LOSS train 1.2239484906196594 valid 1.4486969709396362\n",
      "EPOCH 24\n",
      " batch 10 loss: 1.2191588759422303\n",
      "LOSS train 1.2191588759422303 valid 1.4334299564361572\n",
      "EPOCH 25\n",
      " batch 10 loss: 1.2161818742752075\n",
      "LOSS train 1.2161818742752075 valid 1.4401134252548218\n",
      "EPOCH 26\n",
      " batch 10 loss: 1.211133897304535\n",
      "LOSS train 1.211133897304535 valid 1.4339426755905151\n",
      "EPOCH 27\n",
      " batch 10 loss: 1.2065715789794922\n",
      "LOSS train 1.2065715789794922 valid 1.427415370941162\n",
      "EPOCH 28\n",
      " batch 10 loss: 1.2030719876289369\n",
      "LOSS train 1.2030719876289369 valid 1.4214504957199097\n",
      "EPOCH 29\n",
      " batch 10 loss: 1.1996407628059387\n",
      "LOSS train 1.1996407628059387 valid 1.4180855751037598\n",
      "EPOCH 30\n",
      " batch 10 loss: 1.1952343821525573\n",
      "LOSS train 1.1952343821525573 valid 1.41669499874115\n",
      "EPOCH 31\n",
      " batch 10 loss: 1.1922185897827149\n",
      "LOSS train 1.1922185897827149 valid 1.4127461910247803\n",
      "EPOCH 32\n",
      " batch 10 loss: 1.1887316226959228\n",
      "LOSS train 1.1887316226959228 valid 1.4105277061462402\n",
      "EPOCH 33\n",
      " batch 10 loss: 1.1865807056427002\n",
      "LOSS train 1.1865807056427002 valid 1.409045934677124\n",
      "EPOCH 34\n",
      " batch 10 loss: 1.1815531253814697\n",
      "LOSS train 1.1815531253814697 valid 1.3981775045394897\n",
      "EPOCH 35\n",
      " batch 10 loss: 1.1781147718429565\n",
      "LOSS train 1.1781147718429565 valid 1.3907387256622314\n",
      "EPOCH 36\n",
      " batch 10 loss: 1.174900186061859\n",
      "LOSS train 1.174900186061859 valid 1.3920332193374634\n",
      "EPOCH 37\n",
      " batch 10 loss: 1.1715725660324097\n",
      "LOSS train 1.1715725660324097 valid 1.3898557424545288\n",
      "EPOCH 38\n",
      " batch 10 loss: 1.1683483719825745\n",
      "LOSS train 1.1683483719825745 valid 1.3826429843902588\n",
      "EPOCH 39\n",
      " batch 10 loss: 1.1651596784591676\n",
      "LOSS train 1.1651596784591676 valid 1.3755804300308228\n",
      "EPOCH 40\n",
      " batch 10 loss: 1.162000572681427\n",
      "LOSS train 1.162000572681427 valid 1.3790991306304932\n",
      "EPOCH 41\n",
      " batch 10 loss: 1.1591445565223695\n",
      "LOSS train 1.1591445565223695 valid 1.3756568431854248\n",
      "EPOCH 42\n",
      " batch 10 loss: 1.156064248085022\n",
      "LOSS train 1.156064248085022 valid 1.3751165866851807\n",
      "EPOCH 43\n",
      " batch 10 loss: 1.1534965276718139\n",
      "LOSS train 1.1534965276718139 valid 1.369401454925537\n",
      "EPOCH 44\n",
      " batch 10 loss: 1.150642740726471\n",
      "LOSS train 1.150642740726471 valid 1.3685888051986694\n",
      "EPOCH 45\n",
      " batch 10 loss: 1.1479878425598145\n",
      "LOSS train 1.1479878425598145 valid 1.3627320528030396\n",
      "EPOCH 46\n",
      " batch 10 loss: 1.1453102350234985\n",
      "LOSS train 1.1453102350234985 valid 1.3556721210479736\n",
      "EPOCH 47\n",
      " batch 10 loss: 1.1427086472511292\n",
      "LOSS train 1.1427086472511292 valid 1.353872537612915\n",
      "EPOCH 48\n",
      " batch 10 loss: 1.1400273084640502\n",
      "LOSS train 1.1400273084640502 valid 1.3560771942138672\n",
      "EPOCH 49\n",
      " batch 10 loss: 1.1375274538993836\n",
      "LOSS train 1.1375274538993836 valid 1.355473518371582\n",
      "EPOCH 50\n",
      " batch 10 loss: 1.1354583024978637\n",
      "LOSS train 1.1354583024978637 valid 1.3560361862182617\n"
     ]
    }
   ],
   "source": [
    "epoch_number = 0\n",
    "best_vloss = 1_000_000.\n",
    "losslist = []\n",
    "vlosslist = []\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH {}'.format(epoch_number + 1))\n",
    "    \n",
    "    #Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "    #To do reporting gradients do not need to be on\n",
    "    model.train(False)\n",
    "    \n",
    "    #Pull batches fron validation data to validate\n",
    "    running_vloss = 0\n",
    "    for i, vdata in enumerate(valid_loader):\n",
    "        vinputs, vmasks = vdata\n",
    "        vinputs, vmasks = vinputs.to(device, dtype=torch.float32), vmasks.to(device, dtype=torch.float32)\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vmasks)\n",
    "        running_vloss += vloss\n",
    "    \n",
    "    avg_vloss = running_vloss / (i+1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    #losslist.append(avg_loss)\n",
    "    #vlosslist.append(avg_vloss)\n",
    "    \n",
    "    \n",
    "    #Log the runnig loss averaged per batch for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                      {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                      epoch_number + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "    epoch_number += 1\n",
    "    del vinputs\n",
    "    del vmasks\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca6b1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
